{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We check whether we train with GPU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(torch.cuda.is_available())",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining transformations\n",
    "These transformations are identical to the ones performed on the resnet50 and vgg"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define transforms for the data\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining constansts for the training model\n",
    "These constast are identical to the ones in resnet50 and vgg models that are in comparison."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "EPOCH_COUNT = 20\n",
    "TO_RECOGNIZE = 5\n",
    "\n",
    "TRAIN_PART = 0.8\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 7\n",
    "\n",
    "CHECKPOINT_FILENAME = \"checkpoint.tar\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset:\n",
    "I am using the datasets.ImageFolder method provided by torchvision to create a dataset. It creates a dataset from a folder structure where each class has its own directory containing the images belonging to the class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the data directory\n",
    "data_dir = 'Larger'  # Update this path as needed\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = datasets.ImageFolder(data_dir, transform=None)\n",
    "CLASS_LIST = full_dataset.classes\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "train_idx, test_idx = train_test_split(\n",
    "    list(range(len(full_dataset))), test_size=0.2, stratify=full_dataset.targets)\n",
    "\n",
    "# Create subsets for each set\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "# Apply appropriate transforms to each subset\n",
    "train_dataset.dataset.transform = data_transforms['train']\n",
    "test_dataset.dataset.transform = data_transforms['test']\n",
    "\n",
    "# Create dataloaders for each set\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4),\n",
    "    'test': DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "}\n",
    "\n",
    "# Get dataset sizes\n",
    "dataset_sizes = {\n",
    "    'train': len(train_dataset),\n",
    "    'test': len(test_dataset)\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing meta information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the method to print dataset information\n",
    "def print_dataset_info(full_dataset, train_dataset, test_dataset):\n",
    "    num_classes = len(full_dataset.classes)\n",
    "    print(f\"Total number of samples in the full dataset: {len(full_dataset)}\")\n",
    "    print(f\"Number of samples in the training set: {len(train_dataset)}\")\n",
    "    print(f\"Number of samples in the test set: {len(test_dataset)}\")\n",
    "    print(f\"Total number of classes: {num_classes}\")\n",
    "\n",
    "# Print dataset information\n",
    "print_dataset_info(full_dataset, train_dataset, test_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get class names\n",
    "class_names = full_dataset.classes\n",
    "print(class_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an early stopping method:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showcasing an array of images and their labels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a function to show images\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch with larger padding and bigger images\n",
    "out = torchvision.utils.make_grid(inputs, padding=20, pad_value=1, scale_each=True)\n",
    "\n",
    "# Display batch with labels\n",
    "imshow(out, title=[class_names[x] for x in classes])\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training epoch function:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_train_loss = np.array([], dtype=np.float32)\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Wrap the train_loader with tqdm\n",
    "    train_loader = tqdm(train_loader, desc=\"Training Batches\")\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_train_loss = np.append(running_train_loss, loss.cpu().detach().numpy())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.argmax(outputs, axis=1)\n",
    "\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    train_loss = np.mean(running_train_loss)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return train_loss, train_accuracy\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of metrics for all the predictions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_metrics(correct_predictions, total_predictions, y_true, y_pred):\n",
    "    print('Total predictions: ', total_predictions)\n",
    "    print('Correct predictions: ', correct_predictions)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1, conf_matrix"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of metrics for each of the class with selected treshold"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_metrics_per_class(all_confidences, all_true_labels, threshold=0.8):\n",
    "    if len(all_true_labels.shape) == 1:\n",
    "        all_true_labels = all_true_labels[:, None]\n",
    "\n",
    "    num_classes = all_true_labels.shape[1]\n",
    "    stats = {}\n",
    "    for class_index in range(num_classes):\n",
    "        predictions = (all_confidences[:, class_index] >= threshold).astype(int)\n",
    "        true_labels = all_true_labels[:, class_index].astype(int)\n",
    "\n",
    "        TP = np.sum((predictions == 1) & (true_labels == 1))\n",
    "        TN = np.sum((predictions == 0) & (true_labels == 0))\n",
    "        FP = np.sum((predictions == 1) & (true_labels == 0))\n",
    "        FN = np.sum((predictions == 0) & (true_labels == 1))\n",
    "\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        stats[class_index] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score\n",
    "        }\n",
    "\n",
    "    return stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_epoch(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_val_loss = np.array([], dtype=np.float32)\n",
    "    correct_predictions, total_predictions = 0, 0\n",
    "    y_true, y_pred, all_confidences, all_true_labels = [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            confidences = torch.sigmoid(outputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            running_val_loss = np.append(running_val_loss, val_loss.cpu().detach().numpy())\n",
    "\n",
    "            predicted = torch.argmax(outputs, axis=1)\n",
    "\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "            all_confidences.extend(confidences.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_confidences = np.array(all_confidences)\n",
    "    all_true_labels = np.array(all_true_labels)\n",
    "    \n",
    "    stats = calculate_metrics_per_class(all_confidences, all_true_labels, threshold=0.8)\n",
    "\n",
    "    for class_index, class_stats in stats.items():\n",
    "        print(f\"Class {class_index}:\")\n",
    "        print(f\" Accuracy: {class_stats['accuracy']}\")\n",
    "        print(f\" Precision: {class_stats['precision']}\")\n",
    "        print(f\" Recall: {class_stats['recall']}\")\n",
    "        print(f\" F1 Score: {class_stats['f1_score']}\")\n",
    "\n",
    "    return np.mean(running_val_loss), correct_predictions, total_predictions, y_true, y_pred, stats\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_checkpoint(filename, model, optimizer):\n",
    "    print(\"Loading checkpoint...\")\n",
    "    checkpoint = torch.load(filename)\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    training_history = {\n",
    "        \"epoch\": checkpoint.get(\"epoch\", 0),\n",
    "        \"train_loss_history\": checkpoint.get(\"train_loss_history\", []),\n",
    "        \"accuracy_history\": checkpoint.get(\"accuracy_history\", []),\n",
    "        \"train_accuracy_history\": checkpoint.get(\"train_accuracy_history\", []),\n",
    "        \"val_loss_history\": checkpoint.get(\"val_loss_history\", []),\n",
    "        \"precision_history\": checkpoint.get(\"precision_history\", []),\n",
    "        \"recall_history\": checkpoint.get(\"recall_history\", []),\n",
    "        \"f1_score_history\": checkpoint.get(\"f1_score_history\", []),\n",
    "        \"stats_history\": checkpoint.get(\"stats_history\", []),\n",
    "        \"conf_matrix\": checkpoint.get(\"conf_matrix\", [])\n",
    "    }\n",
    "\n",
    "    return training_history\n",
    "\n",
    "def seconds_to_time(seconds):\n",
    "    s = int(seconds) % 60\n",
    "    m = int(seconds) // 60\n",
    "    if m < 1:\n",
    "        return f'{s}s'\n",
    "    h = m // 60\n",
    "    m = m % 60\n",
    "    if h < 1:\n",
    "        return f'{m}m{s}s'\n",
    "    return f'{h}h{m}m{s}s'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and finetuning model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load a pretrained MobileNetV2 model\n",
    "model_ft = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Modify the classifier to match the number of classes\n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.classifier[1] = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model_ft = model_ft.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "\n",
    "def train_and_eval(model, loader_train, loader_valid, filename, epoch_count, lr):\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model_ft.features.parameters(), 'lr': lr * 0.1},\n",
    "        {'params': model_ft.classifier.parameters(), 'lr': lr}\n",
    "    ])\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, min_delta=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=PATIENCE, verbose=True)\n",
    "\n",
    "    training_history = None\n",
    "\n",
    "    if LOAD_CHECKPOINT:\n",
    "        training_history = load_checkpoint(filename, model, optimizer)\n",
    "        start_epoch = training_history['epoch'] + 1\n",
    "        train_loss_history = training_history['train_loss_history']\n",
    "        accuracy_history = training_history['accuracy_history']\n",
    "        train_accuracy_history = training_history['train_accuracy_history']\n",
    "        val_loss_history = training_history['val_loss_history']\n",
    "        precision_history = training_history['precision_history']\n",
    "        recall_history = training_history['recall_history']\n",
    "        f1_score_history = training_history['f1_score_history']\n",
    "        stats_history = training_history['stats_history']\n",
    "        conf_matrix = training_history['conf_matrix']\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        accuracy_history = []\n",
    "        train_accuracy_history = []\n",
    "        precision_history = []\n",
    "        recall_history = []\n",
    "        f1_score_history = []\n",
    "        stats_history = []\n",
    "        conf_matrix = []\n",
    "\n",
    "    for epoch in range(start_epoch, epoch_count):\n",
    "        print('Starting training epoch... ', epoch)\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        train_loss, train_accuracy = train_epoch(model, loader_train, loss_func, optimizer)\n",
    "\n",
    "        current_time = datetime.now()\n",
    "        elapsed = seconds_to_time((current_time - start_time).total_seconds())\n",
    "        print(f'Epoch: {epoch}, Time: {elapsed}, Loss: {train_loss}')\n",
    "\n",
    "        print('Starting evaluation... ', start_time)\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        avg_val_loss, correct_predictions, total_predictions, y_true, y_pred, stats = evaluate_epoch(model, dataloaders[\"test\"], loss_func)\n",
    "\n",
    "        accuracy, precision, recall, f1, conf_matrix = calculate_metrics(correct_predictions, total_predictions, y_true, y_pred)\n",
    "\n",
    "        current_time = datetime.now()\n",
    "        per_image = (current_time - start_time).total_seconds() / total_predictions\n",
    "        print(f'Time: {per_image * 1000}ms, Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 score: {f1:.4f}')\n",
    "        print(f'Confusion matrix: {conf_matrix}')\n",
    "\n",
    "        accuracy_history.append(accuracy)\n",
    "        train_accuracy_history.append(train_accuracy)\n",
    "        train_loss_history.append(train_loss)\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        precision_history.append(precision)\n",
    "        recall_history.append(recall)\n",
    "        f1_score_history.append(f1)\n",
    "        stats_history.append(stats)\n",
    "\n",
    "        print(\"Saving checkpoint...\")\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"train_loss_history\": train_loss_history,\n",
    "            \"accuracy_history\": accuracy_history,\n",
    "            \"train_accuracy_history\": train_accuracy_history,\n",
    "            \"val_loss_history\": val_loss_history,\n",
    "            \"precision_history\": precision_history,\n",
    "            \"recall_history\": recall_history,\n",
    "            \"f1_score_history\": f1_score_history,\n",
    "            \"stats_history\": stats_history,\n",
    "            \"conf_matrix\": conf_matrix\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        early_stopping(avg_val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered. Reducing learning rate and resetting early stopping.\")\n",
    "            early_stopping.counter = 0\n",
    "\n",
    "    return training_history"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching of the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f'Parameter count: {sum(p.numel() for p in model_ft.parameters() if p.requires_grad):,}')\n",
    "training_history = train_and_eval(model_ft, dataloaders[\"train\"], dataloaders[\"test\"], CHECKPOINT_FILENAME, epoch_count = EPOCH_COUNT, lr = LEARNING_RATE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display of statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "optimizer = optim.Adam([\n",
    "        {'params': model_ft.features.parameters(), 'lr': LEARNING_RATE * 0.1},\n",
    "        {'params': model_ft.classifier.parameters(), 'lr': LEARNING_RATE}\n",
    "])\n",
    "\n",
    "# Load a pretrained MobileNetV2 model\n",
    "model_ft = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Modify the classifier to match the number of classes\n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.classifier[1] = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "training_history = load_checkpoint(CHECKPOINT_FILENAME, model_ft, optimizer)\n",
    "\n",
    "train_accuracy_history = training_history['train_accuracy_history']\n",
    "accuracy_history = training_history['accuracy_history']\n",
    "train_loss_history = training_history['train_loss_history']\n",
    "val_loss_history = training_history['val_loss_history']\n",
    "conf_matrix = training_history['conf_matrix']\n",
    "\n",
    "plt.plot(train_accuracy_history, label='Train Accuracy')\n",
    "plt.plot(accuracy_history, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "if isinstance(conf_matrix, list):\n",
    "    last_conf_matrix = np.array(conf_matrix[-1])\n",
    "else:\n",
    "    last_conf_matrix = np.array(conf_matrix)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=last_conf_matrix, display_labels=full_dataset.classes)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poultry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
