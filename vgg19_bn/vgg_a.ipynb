{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true,
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "5CvPfjNEA6C2",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.519162Z",
     "start_time": "2024-05-18T17:29:32.761636Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Konstantos**\n"
   ],
   "metadata": {
    "id": "CV0gLdtjHjo0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.   ```LOAD_CHECKPOINT``` - switch to True to load existing checkpoint from path\n",
    "2.   ```USE_SUBSET``` - switch to True to use a part of dataset (useful for testing purposes)\n",
    "3. ```TRAIN_SUBSET_SIZE``` - define size of train part of dataset\n",
    "4. ```VALID_SUBSET_SIZE``` - define a size of test part of dataset"
   ],
   "metadata": {
    "id": "u9FHXSidHpzb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**WARNING!!! IMPORTANT!!!**\n",
    "\n",
    "\n",
    "1.   Jei norite bandyti trainint (patestuot) ir nežinote ar gerai bus, geriausia pakeiskite ```CHECKPOINT_FILENAME``` į kokį nors kitą, kad neprarastume rezultatus. ```checkpoint_poultry``` checkpointa naudokime kaip final versiją.\n",
    "\n"
   ],
   "metadata": {
    "id": "b8COkzCgJw7R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "FEATURE_EXTRACTION = True\n",
    "CLASS_LIST = [\"healthy\", \"salmo\", \"cocci\", \"ncd\"]\n",
    "CLASS_INDICES = {'healthy': 0, 'salmo': 1, 'cocci': 2, 'ncd': 3}\n",
    "EPOCH_COUNT = 50\n",
    "TO_RECOGNIZE = 5\n",
    "\n",
    "TRAIN_PART = 0.8\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 12\n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "ROOT_DIR = \"../data\"\n",
    "CHECKPOINT_FILENAME = \"./checkpoint_poultry.tar\"\n",
    "\n",
    "USE_SUBSET = False\n",
    "TRAIN_SUBSET_SIZE = 50 * len(CLASS_LIST)\n",
    "VALID_SUBSET_SIZE = 15 * len(CLASS_LIST)\n",
    "\n",
    "device = torch.device('mps')\n",
    "device"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTGJ7y6JR_j1",
    "outputId": "c120670d-6ac8-4b34-812f-381a9aaa486b",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.523321Z",
     "start_time": "2024-05-18T17:29:34.520142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Some helper functions**"
   ],
   "metadata": {
    "id": "JlVrZQCCJUc6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def seconds_to_time(seconds):\n",
    "    s = int(seconds) % 60\n",
    "    m = int(seconds) // 60\n",
    "    if m < 1:\n",
    "        return f'{s}s'\n",
    "    h = m // 60\n",
    "    m = m % 60\n",
    "    if h < 1:\n",
    "        return f'{m}m{s}s'\n",
    "    return f'{h}h{m}m{s}s'"
   ],
   "metadata": {
    "id": "Wium8j3dfrcB",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.525526Z",
     "start_time": "2024-05-18T17:29:34.523861Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "def load_checkpoint(filename, model, optimizer):\n",
    "    print(\"Loading checkpoint...\")\n",
    "    print(filename)\n",
    "    checkpoint = torch.load(filename)\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    training_history = {\n",
    "        \"epoch\": checkpoint.get(\"epoch\", 0),\n",
    "        \"train_loss_history\": checkpoint.get(\"train_loss_history\", []),\n",
    "        \"accuracy_history\": checkpoint.get(\"accuracy_history\", []),\n",
    "        \"train_accuracy_history\": checkpoint.get(\"train_accuracy_history\", []),\n",
    "        \"val_loss_history\": checkpoint.get(\"val_loss_history\", []),\n",
    "        \"precision_history\": checkpoint.get(\"precision_history\", []),\n",
    "        \"recall_history\": checkpoint.get(\"recall_history\", []),\n",
    "        \"f1_score_history\": checkpoint.get(\"f1_score_history\", []),\n",
    "        \"stats_history\": checkpoint.get(\"stats_history\", []),\n",
    "        \"conf_matrix\": checkpoint.get(\"conf_matrix\", [])\n",
    "    }\n",
    "\n",
    "    return training_history"
   ],
   "metadata": {
    "id": "Vz80tWulfsJT",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.527957Z",
     "start_time": "2024-05-18T17:29:34.526046Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Early stopping**"
   ],
   "metadata": {
    "id": "IJnfM5wYVM1W"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0"
   ],
   "metadata": {
    "id": "o19MG9cBVP03",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.530861Z",
     "start_time": "2024-05-18T17:29:34.529074Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Vištų šūdukų datasetas paruoštas klasifikavimo uždaviniui**\n",
    "        "
   ],
   "metadata": {
    "id": "nHogcEHHITgn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from poultry_dataset import PoultryDatasetClassification\n",
    "\n",
    "def create_datasets(root_dir, data_classes, test_size=0.2, random_state=42):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_indices = {data_class: i for i, data_class in enumerate(data_classes)}\n",
    "\n",
    "    for data_class in data_classes:\n",
    "        images_dir = f\"{root_dir}/{data_class}\"\n",
    "        print(images_dir)\n",
    "        if not os.path.exists(images_dir):\n",
    "            print(f\"Directory does not exist: {images_dir}\")\n",
    "            continue\n",
    "\n",
    "        for img in os.listdir(images_dir):\n",
    "            if img.endswith(\".jpg\"):\n",
    "                image_paths.append(os.path.join(images_dir, img))\n",
    "                label = [0] * len(data_classes)\n",
    "                label[class_indices[data_class]] = 1\n",
    "                labels.append(label)\n",
    "\n",
    "    # Split data while keeping the distribution of classes consistent\n",
    "    img_train, img_test, labels_train, labels_test = train_test_split(\n",
    "        image_paths, labels, test_size=test_size, random_state=random_state, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Applying transformations\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Creating dataset objects for training and testing\n",
    "    train_dataset = PoultryDatasetClassification(img_train, labels_train, transform=transform_train)\n",
    "    test_dataset = PoultryDatasetClassification(img_test, labels_test, transform=transform_test)\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ],
   "metadata": {
    "id": "JAuuwK-RVyDC",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.534975Z",
     "start_time": "2024-05-18T17:29:34.531573Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_train_loss = np.array([], dtype=np.float32)\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels_as_indices = torch.argmax(labels, dim=1)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels_as_indices)\n",
    "        running_train_loss = np.append(running_train_loss, loss.cpu().detach().numpy())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.argmax(outputs, axis = 1)\n",
    "        correct_predictions += (predicted == labels_as_indices).sum().item()\n",
    "        total_predictions += labels_as_indices.size(0)\n",
    "\n",
    "    train_loss = np.mean(running_train_loss)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return train_loss, train_accuracy"
   ],
   "metadata": {
    "id": "aUr7WiOYQZJc",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.537700Z",
     "start_time": "2024-05-18T17:29:34.535479Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Skaičiuojamos metrikos visiems predictionams**"
   ],
   "metadata": {
    "id": "SDxTycLHIk1E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_metrics(correct_predictions, total_predictions, y_true, y_pred):\n",
    "    print('Total predictions: ', total_predictions)\n",
    "    print('Correct predictions: ', correct_predictions)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1, conf_matrix"
   ],
   "metadata": {
    "id": "RLVv08mrYFUu",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.539869Z",
     "start_time": "2024-05-18T17:29:34.538324Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Skaičiuojamos metrikos kiekvienai klasei su pasirinktu threshold**"
   ],
   "metadata": {
    "id": "L80fJFJbIcxF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_metrics_per_class(all_confidences, all_true_labels, threshold=0.8):\n",
    "    num_classes = all_true_labels.shape[1]\n",
    "    stats = {}\n",
    "    for class_index in range(num_classes):\n",
    "        predictions = (all_confidences[:, class_index] >= threshold).astype(int)\n",
    "\n",
    "        true_labels = all_true_labels[:, class_index].astype(int)\n",
    "\n",
    "        TP = np.sum((predictions == 1) & (true_labels == 1))\n",
    "        TN = np.sum((predictions == 0) & (true_labels == 0))\n",
    "        FP = np.sum((predictions == 1) & (true_labels == 0))\n",
    "        FN = np.sum((predictions == 0) & (true_labels == 1))\n",
    "\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        stats[class_index] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score\n",
    "        }\n",
    "\n",
    "    return stats"
   ],
   "metadata": {
    "id": "spyTm781qmjR",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.542771Z",
     "start_time": "2024-05-18T17:29:34.540428Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Evaluation loop'as**"
   ],
   "metadata": {
    "id": "yRxnHzXOJHJ_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_epoch(model, test_loader, criterion, optimizer):\n",
    "    model.eval()\n",
    "    running_val_loss = np.array([], dtype = np.float32)\n",
    "    correct_predictions, total_predictions = 0, 0\n",
    "    y_true, y_pred, all_confidences, all_true_labels = [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels_as_indices = torch.argmax(labels, axis=1).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            confidences = torch.sigmoid(outputs)\n",
    "\n",
    "            val_loss = criterion(outputs, labels_as_indices)\n",
    "            running_val_loss = np.append(running_val_loss, val_loss.cpu().detach().numpy())\n",
    "\n",
    "            predicted = torch.argmax(outputs, axis = 1)\n",
    "\n",
    "            correct_predictions += (predicted == labels_as_indices).sum().item()\n",
    "            total_predictions += labels_as_indices.size(0)\n",
    "\n",
    "            y_true.extend(labels_as_indices.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "            all_confidences.extend(confidences.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_confidences = np.array(all_confidences)\n",
    "    all_true_labels = np.array(all_true_labels)\n",
    "\n",
    "    stats = calculate_metrics_per_class(all_confidences, all_true_labels, threshold=0.5)\n",
    "\n",
    "    for class_index, class_stats in stats.items():\n",
    "        print(f\"Class {class_index}:\")\n",
    "        print(f\" Accuracy: {class_stats['accuracy']}\")\n",
    "        print(f\" Precision: {class_stats['precision']}\")\n",
    "        print(f\" Recall: {class_stats['recall']}\")\n",
    "        print(f\" F1 Score: {class_stats['f1_score']}\")\n",
    "\n",
    "    return np.mean(running_val_loss), correct_predictions, total_predictions, y_true, y_pred, stats"
   ],
   "metadata": {
    "id": "sxOpZq8vX9Sn",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.546328Z",
     "start_time": "2024-05-18T17:29:34.543342Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pagrindnis loop'as, išsaugom istoriją kad ateityje būtų galima nubraižyt fancy grafikelius**"
   ],
   "metadata": {
    "id": "x4UCtGgqIt_n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_and_eval(model, loader_train, loader_valid, filename, epoch_count, lr):\n",
    "  loss_func = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "  early_stopping = EarlyStopping(patience=PATIENCE, min_delta=0.01)\n",
    "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=PATIENCE, verbose=True)\n",
    "  start_time = datetime.now()\n",
    "\n",
    "  training_history = None\n",
    "\n",
    "  if LOAD_CHECKPOINT:\n",
    "      training_history = load_checkpoint(filename, model, optimizer)\n",
    "      start_epoch = training_history['epoch'] + 1\n",
    "      train_loss_history = training_history['train_loss_history']\n",
    "      accuracy_history = training_history['accuracy_history']\n",
    "      train_accuracy_history = training_history['train_accuracy_history']\n",
    "      val_loss_history = training_history['val_loss_history']\n",
    "      precision_history = training_history['precision_history']\n",
    "      recall_history = training_history['recall_history']\n",
    "      f1_score_history = training_history['f1_score_history']\n",
    "      stats_history = training_history['stats_history']\n",
    "      conf_matrix = training_history['conf_matrix']\n",
    "  else:\n",
    "      start_epoch = 0\n",
    "      train_loss_history = []\n",
    "      val_loss_history = []\n",
    "      accuracy_history = []\n",
    "      train_accuracy_history = []\n",
    "      precision_history = []\n",
    "      recall_history = []\n",
    "      f1_score_history = []\n",
    "      stats_history = []\n",
    "      conf_matrix = []\n",
    "\n",
    "  for epoch in range(start_epoch, epoch_count):\n",
    "      print('Starting training epoch... ', epoch)\n",
    "      start_time = datetime.now()\n",
    "\n",
    "      train_loss, train_accuracy = train_epoch(model, loader_train, loss_func, optimizer)\n",
    "\n",
    "      current_time = datetime.now()\n",
    "      elapsed = seconds_to_time((current_time - start_time).total_seconds())\n",
    "      print(f'Epoch: {epoch}, Time: {elapsed}, Loss: {train_loss}')\n",
    "\n",
    "      print('Starting evaluation... ', start_time)\n",
    "      start_time = datetime.now()\n",
    "\n",
    "      avg_val_loss, correct_predictions, total_predictions, y_true, y_pred, stats = evaluate_epoch(model, test_loader, loss_func, optimizer)\n",
    "\n",
    "      accuracy, precision, recall, f1, conf_matrix = calculate_metrics(correct_predictions, total_predictions, y_true, y_pred)\n",
    "\n",
    "      current_time = datetime.now()\n",
    "      per_image = (current_time - start_time).total_seconds() / total_predictions\n",
    "      print(f'Time: {per_image * 1000}ms, Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 score: {f1:.4f}')\n",
    "      print(f'Confusion matrix: {conf_matrix}')\n",
    "\n",
    "      accuracy_history.append(accuracy)\n",
    "      train_accuracy_history.append(train_accuracy)\n",
    "      train_loss_history.append(train_loss)\n",
    "      val_loss_history.append(avg_val_loss)\n",
    "      precision_history.append(precision)\n",
    "      recall_history.append(recall)\n",
    "      f1_score_history.append(f1)\n",
    "      stats_history.append(stats)\n",
    "\n",
    "      print(\"Saving checkpoint...\")\n",
    "      checkpoint = {\n",
    "          \"epoch\": epoch,\n",
    "          \"state_dict\": model.state_dict(),\n",
    "          \"optimizer\": optimizer.state_dict(),\n",
    "          \"train_loss_history\": train_loss_history,\n",
    "          \"accuracy_history\": accuracy_history,\n",
    "          \"train_accuracy_history\": train_accuracy_history,\n",
    "          \"val_loss_history\": val_loss_history,\n",
    "          \"precision_history\": precision_history,\n",
    "          \"recall_history\": recall_history,\n",
    "          \"f1_score_history\": f1_score_history,\n",
    "          \"stats_history\": stats_history,\n",
    "          \"conf_matrix\": conf_matrix\n",
    "      }\n",
    "      torch.save(checkpoint, filename)\n",
    "\n",
    "      scheduler.step(avg_val_loss)\n",
    "\n",
    "      early_stopping(avg_val_loss)\n",
    "      if early_stopping.early_stop:\n",
    "          print(\"Early stopping triggered. Reducing learning rate and resetting early stopping.\")\n",
    "          early_stopping.early_stop = False\n",
    "          early_stopping.counter = 0\n",
    "\n",
    "  return training_history\n"
   ],
   "metadata": {
    "id": "92y_R0mPQd7L",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.551484Z",
     "start_time": "2024-05-18T17:29:34.547082Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset, test_dataset = create_datasets(ROOT_DIR, CLASS_LIST)\n",
    "\n",
    "if USE_SUBSET:\n",
    "  train_dataset = Subset(train_dataset, range(TRAIN_SUBSET_SIZE))\n",
    "  test_dataset = Subset(test_dataset, range(VALID_SUBSET_SIZE))\n",
    "\n",
    "print(f'Train: {len(train_dataset)}, Test: {len(test_dataset)}')\n",
    "\n",
    "num_workers = 2\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN81usApf92Z",
    "outputId": "4dd850ff-bec9-4dbb-9d48-b38592169713",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.576264Z",
     "start_time": "2024-05-18T17:29:34.552080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/healthy\n",
      "../data/salmo\n",
      "../data/cocci\n",
      "../data/ncd\n",
      "Train: 5449, Test: 1363\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ResNet50 model modification**"
   ],
   "metadata": {
    "id": "2jDVGXtXKS4X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class PoultryDiseaseClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoultryDiseaseClassifier, self).__init__()\n",
    "        self.resnet50 = resnet50\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(num_ftrs, 256)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.dense2 = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "znqhYWnzQWbN",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.578695Z",
     "start_time": "2024-05-18T17:29:34.576798Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": "**Kviečiam mokymą**",
   "metadata": {
    "id": "G78c7hWEKo5n"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T17:29:34.582459Z",
     "start_time": "2024-05-18T17:29:34.580873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.models.vgg as vgg\n",
    "\n",
    "def get_model():\n",
    "    model = vgg.vgg19_bn(weights = vgg.VGG19_BN_Weights.DEFAULT)\n",
    "    \n",
    "    # Modify the classifier to match the number of classes\n",
    "    num_ftrs = model.classifier[6].in_features\n",
    "    model.classifier[6] = nn.Linear(num_ftrs, len(CLASS_LIST))\n",
    "    \n",
    "    return model\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T17:35:40.556430Z",
     "start_time": "2024-05-18T17:29:34.582890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.models.vgg as vgg\n",
    "\n",
    "model = get_model().to(device)\n",
    "\n",
    "print(f'Parameter count: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')\n",
    "training_history = train_and_eval(model, train_loader, test_loader, CHECKPOINT_FILENAME, epoch_count = EPOCH_COUNT, lr = LEARNING_RATE)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count: 139,597,636\n",
      "Starting training epoch...  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/domantas.keturakis/Projects/PoultryDiseaseDetection/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 3m26s, Loss: 1.0558011531829834\n",
      "Starting evaluation...  2024-05-18 20:29:35.177768\n",
      "Class 0:\n",
      " Accuracy: 0.3521643433602348\n",
      " Precision: 0.31814671814671813\n",
      " Recall: 1.0\n",
      " F1 Score: 0.4827182190978324\n",
      "Class 1:\n",
      " Accuracy: 0.3565663976522377\n",
      " Precision: 0.3415915915915916\n",
      " Recall: 1.0\n",
      " F1 Score: 0.5092333519865697\n",
      "Class 2:\n",
      " Accuracy: 0.7373440939104916\n",
      " Precision: 1.0\n",
      " Recall: 0.1496437054631829\n",
      " F1 Score: 0.26033057851239666\n",
      "Class 3:\n",
      " Accuracy: 0.9449743213499633\n",
      " Precision: 0\n",
      " Recall: 0.0\n",
      " F1 Score: 0\n",
      "Total predictions:  1363\n",
      "Correct predictions:  511\n",
      "Time: 22.19528760088041ms, Epoch 0, Train Loss: 1.0558, Val Loss: 1.5612, Accuracy: 0.3749, Precision: 0.4251, Recall: 0.3749, F1 score: 0.2449\n",
      "Confusion matrix: [[  0 412   0   0]\n",
      " [  0 455   0   0]\n",
      " [  0 365  56   0]\n",
      " [  0  75   0   0]]\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/domantas.keturakis/Projects/PoultryDiseaseDetection/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch...  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m get_model()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mParameter count: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28msum\u001B[39m(p\u001B[38;5;241m.\u001B[39mnumel()\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mp\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mmodel\u001B[38;5;241m.\u001B[39mparameters()\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39mp\u001B[38;5;241m.\u001B[39mrequires_grad)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m training_history \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCHECKPOINT_FILENAME\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch_count\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mEPOCH_COUNT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mLEARNING_RATE\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[11], line 38\u001B[0m, in \u001B[0;36mtrain_and_eval\u001B[0;34m(model, loader_train, loader_valid, filename, epoch_count, lr)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStarting training epoch... \u001B[39m\u001B[38;5;124m'\u001B[39m, epoch)\n\u001B[1;32m     36\u001B[0m start_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[0;32m---> 38\u001B[0m train_loss, train_accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloader_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m current_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[1;32m     41\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m seconds_to_time((current_time \u001B[38;5;241m-\u001B[39m start_time)\u001B[38;5;241m.\u001B[39mtotal_seconds())\n",
      "Cell \u001B[0;32mIn[7], line 22\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, train_loader, criterion, optimizer)\u001B[0m\n\u001B[1;32m     19\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     21\u001B[0m     predicted \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(outputs, axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 22\u001B[0m     correct_predictions \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mpredicted\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlabels_as_indices\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m     total_predictions \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m labels_as_indices\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     25\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(running_train_loss)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Statistiku atvaizdavimas**"
   ],
   "metadata": {
    "id": "89VFfJK2Pdgo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = get_model().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "training_history = load_checkpoint(CHECKPOINT_FILENAME, model, optimizer)\n",
    "\n",
    "train_accuracy_history = training_history['train_accuracy_history']\n",
    "accuracy_history = training_history['accuracy_history']\n",
    "train_loss_history = training_history['train_loss_history']\n",
    "val_loss_history = training_history['val_loss_history']\n",
    "conf_matrix = training_history['conf_matrix']\n",
    "\n",
    "plt.plot(train_accuracy_history, label='Train Accuracy')\n",
    "plt.plot(accuracy_history, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "if isinstance(conf_matrix, list):\n",
    "    last_conf_matrix = np.array(conf_matrix[-1])\n",
    "else:\n",
    "    last_conf_matrix = np.array(conf_matrix)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=last_conf_matrix, display_labels=CLASS_LIST)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Finding the epoch with the highest validation accuracy\n",
    "\n",
    "# Finding the epoch with the lowest validation loss\n",
    "best_epoch_loss = np.argmin(val_loss_history)\n",
    "best_val_loss = val_loss_history[best_epoch_loss]\n",
    "\n",
    "best_epoch_accuracy = np.argmax(accuracy_history)\n",
    "best_val_accuracy = accuracy_history[best_epoch_accuracy]\n",
    "\n",
    "print(f'Best epoch based on validation accuracy: {best_epoch_accuracy} with accuracy: {best_val_accuracy:.4f}')\n",
    "print(f'Best epoch based on validation loss: {best_epoch_loss} with loss: {best_val_loss:.4f}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mFW0StkhPhb2",
    "outputId": "42210b50-5b8a-48f5-9415-5757990e77b9",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:35:40.557142Z",
     "start_time": "2024-05-18T17:35:40.557094Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Transformaciju atvaizdavimas**"
   ],
   "metadata": {
    "id": "ErbgYuZzge2u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def denormalize(img, mean, std):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    img = img * std + mean\n",
    "    return img\n",
    "\n",
    "def imshow(img, title=None):\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.clip(npimg, 0, 1)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "INDEX_TO_CLASS = {v: k for k, v in CLASS_INDICES.items()}\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "num_images_to_show = 5\n",
    "fig, axes = plt.subplots(1, num_images_to_show, figsize=(15, 3))\n",
    "for i in range(num_images_to_show):\n",
    "    ax = axes[i]\n",
    "    img = denormalize(images[i], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.clip(npimg, 0, 1)\n",
    "    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    class_index = labels[i].argmax().item()\n",
    "    class_name = INDEX_TO_CLASS[class_index]\n",
    "    ax.set_title(f\"Label: {class_name}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "x_qA5Cyhgimm",
    "outputId": "b8f30aef-8c17-48f1-af01-23491a7b5b6c"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
