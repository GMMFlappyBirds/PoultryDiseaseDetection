{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "5CvPfjNEA6C2",
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.435645Z",
     "start_time": "2024-05-18T16:59:37.486241Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Konstantos**\n"
   ],
   "metadata": {
    "id": "CV0gLdtjHjo0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.   ```LOAD_CHECKPOINT``` - switch to True to load existing checkpoint from path\n",
    "2.   ```USE_SUBSET``` - switch to True to use a part of dataset (useful for testing purposes)\n",
    "3. ```TRAIN_SUBSET_SIZE``` - define size of train part of dataset\n",
    "4. ```VALID_SUBSET_SIZE``` - define a size of test part of dataset"
   ],
   "metadata": {
    "id": "u9FHXSidHpzb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**WARNING!!! IMPORTANT!!!**\n",
    "\n",
    "\n",
    "1.   Jei norite bandyti trainint (patestuot) ir nežinote ar gerai bus, geriausia pakeiskite ```CHECKPOINT_FILENAME``` į kokį nors kitą, kad neprarastume rezultatus. ```checkpoint_poultry``` checkpointa naudokime kaip final versiją.\n",
    "\n"
   ],
   "metadata": {
    "id": "b8COkzCgJw7R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "FEATURE_EXTRACTION = True\n",
    "CLASS_LIST = [\"healthy\", \"salmo\", \"cocci\", \"ncd\"]\n",
    "CLASS_INDICES = {'healthy': 0, 'salmo': 1, 'cocci': 2, 'ncd': 3}\n",
    "EPOCH_COUNT = 20\n",
    "TO_RECOGNIZE = 5\n",
    "\n",
    "TRAIN_PART = 0.8\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 7\n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "ROOT_DIR = \"../data\"\n",
    "CHECKPOINT_FILENAME = \"./checkpoint_poultry.tar\"\n",
    "\n",
    "USE_SUBSET = False\n",
    "TRAIN_SUBSET_SIZE = 50 * len(CLASS_LIST)\n",
    "VALID_SUBSET_SIZE = 15 * len(CLASS_LIST)\n",
    "\n",
    "device = torch.device('mps')\n",
    "print(f'Device: {device}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTGJ7y6JR_j1",
    "outputId": "c538fb42-12ba-4457-98b4-7d586e1e2b16",
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.438891Z",
     "start_time": "2024-05-18T16:59:39.436583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Some helper functions**"
   ],
   "metadata": {
    "id": "JlVrZQCCJUc6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def seconds_to_time(seconds):\n",
    "    s = int(seconds) % 60\n",
    "    m = int(seconds) // 60\n",
    "    if m < 1:\n",
    "        return f'{s}s'\n",
    "    h = m // 60\n",
    "    m = m % 60\n",
    "    if h < 1:\n",
    "        return f'{m}m{s}s'\n",
    "    return f'{h}h{m}m{s}s'"
   ],
   "metadata": {
    "id": "Wium8j3dfrcB",
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.441069Z",
     "start_time": "2024-05-18T16:59:39.439530Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "def load_checkpoint(filename, model, optimizer):\n",
    "    print(\"Loading checkpoint...\")\n",
    "    checkpoint = torch.load(filename)\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    training_history = {\n",
    "        \"epoch\": checkpoint.get(\"epoch\", 0),\n",
    "        \"train_loss_history\": checkpoint.get(\"train_loss_history\", []),\n",
    "        \"accuracy_history\": checkpoint.get(\"accuracy_history\", []),\n",
    "        \"train_accuracy_history\": checkpoint.get(\"train_accuracy_history\", []),\n",
    "        \"val_loss_history\": checkpoint.get(\"val_loss_history\", []),\n",
    "        \"precision_history\": checkpoint.get(\"precision_history\", []),\n",
    "        \"recall_history\": checkpoint.get(\"recall_history\", []),\n",
    "        \"f1_score_history\": checkpoint.get(\"f1_score_history\", []),\n",
    "        \"stats_history\": checkpoint.get(\"stats_history\", []),\n",
    "        \"conf_matrix\": checkpoint.get(\"conf_matrix\", [])\n",
    "    }\n",
    "\n",
    "    return training_history"
   ],
   "metadata": {
    "id": "Vz80tWulfsJT",
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.443253Z",
     "start_time": "2024-05-18T16:59:39.441501Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Early stopping**"
   ],
   "metadata": {
    "id": "IJnfM5wYVM1W"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0"
   ],
   "metadata": {
    "id": "o19MG9cBVP03",
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.446321Z",
     "start_time": "2024-05-18T16:59:39.444515Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "**Vištų šūdukų datasetas paruoštas klasifikavimo uždaviniui**",
   "metadata": {
    "id": "nHogcEHHITgn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from poultry_dataset import PoultryDatasetClassification\n",
    "\n",
    "def create_datasets(root_dir, data_classes, test_size=0.2, random_state=42):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_indices = {data_class: i for i, data_class in enumerate(data_classes)}\n",
    "\n",
    "    for data_class in data_classes:\n",
    "        # images_dir = os.path.join(root_dir, data_class, data_class)\n",
    "        images_dir = f\"{root_dir}/{data_class}\"\n",
    "        print(images_dir)\n",
    "        if not os.path.exists(images_dir):\n",
    "            print(f\"Directory does not exist: {images_dir}\")\n",
    "            continue\n",
    "\n",
    "        for img in os.listdir(images_dir):\n",
    "            if img.endswith(\".jpg\"):\n",
    "                image_paths.append(os.path.join(images_dir, img))\n",
    "                label = [0] * len(data_classes)\n",
    "                label[class_indices[data_class]] = 1\n",
    "                labels.append(label)\n",
    "\n",
    "    # Split data while keeping the distribution of classes consistent\n",
    "    img_train, img_test, labels_train, labels_test = train_test_split(\n",
    "        image_paths, labels, test_size=test_size, random_state=random_state, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Applying transformations\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Creating dataset objects for training and testing\n",
    "    train_dataset = PoultryDatasetClassification(img_train, labels_train, transform=transform_train)\n",
    "    test_dataset = PoultryDatasetClassification(img_test, labels_test, transform=transform_test)\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ],
   "metadata": {
    "id": "JAuuwK-RVyDC",
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.450626Z",
     "start_time": "2024-05-18T16:59:39.446816Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_train_loss = np.array([], dtype=np.float32)\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels_as_indices = torch.argmax(labels, dim=1)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels_as_indices)\n",
    "        running_train_loss = np.append(running_train_loss, loss.cpu().detach().numpy())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.argmax(outputs, axis = 1)\n",
    "        correct_predictions += (predicted == labels_as_indices).sum().item()\n",
    "        total_predictions += labels_as_indices.size(0)\n",
    "\n",
    "    train_loss = np.mean(running_train_loss)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return train_loss, train_accuracy"
   ],
   "metadata": {
    "id": "aUr7WiOYQZJc",
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.453227Z",
     "start_time": "2024-05-18T16:59:39.451162Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Skaičiuojamos metrikos visiems predictionams**"
   ],
   "metadata": {
    "id": "SDxTycLHIk1E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_metrics(correct_predictions, total_predictions, y_true, y_pred):\n",
    "    print('Total predictions: ', total_predictions)\n",
    "    print('Correct predictions: ', correct_predictions)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1, conf_matrix"
   ],
   "metadata": {
    "id": "RLVv08mrYFUu",
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.455422Z",
     "start_time": "2024-05-18T16:59:39.453853Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Skaičiuojamos metrikos kiekvienai klasei su pasirinktu threshold**"
   ],
   "metadata": {
    "id": "L80fJFJbIcxF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_metrics_per_class(all_confidences, all_true_labels, threshold=0.8):\n",
    "    num_classes = all_true_labels.shape[1]\n",
    "    stats = {}\n",
    "    for class_index in range(num_classes):\n",
    "        predictions = (all_confidences[:, class_index] >= threshold).astype(int)\n",
    "\n",
    "        true_labels = all_true_labels[:, class_index].astype(int)\n",
    "\n",
    "        TP = np.sum((predictions == 1) & (true_labels == 1))\n",
    "        TN = np.sum((predictions == 0) & (true_labels == 0))\n",
    "        FP = np.sum((predictions == 1) & (true_labels == 0))\n",
    "        FN = np.sum((predictions == 0) & (true_labels == 1))\n",
    "\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        stats[class_index] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score\n",
    "        }\n",
    "\n",
    "    return stats"
   ],
   "metadata": {
    "id": "spyTm781qmjR",
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.458133Z",
     "start_time": "2024-05-18T16:59:39.455992Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Evaluation loop'as**"
   ],
   "metadata": {
    "id": "yRxnHzXOJHJ_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_epoch(model, test_loader, criterion, optimizer):\n",
    "    model.eval()\n",
    "    running_val_loss = np.array([], dtype = np.float32)\n",
    "    correct_predictions, total_predictions = 0, 0\n",
    "    y_true, y_pred, all_confidences, all_true_labels = [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels_as_indices = torch.argmax(labels, axis=1).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            confidences = torch.sigmoid(outputs)\n",
    "\n",
    "            val_loss = criterion(outputs, labels_as_indices)\n",
    "            running_val_loss = np.append(running_val_loss, val_loss.cpu().detach().numpy())\n",
    "\n",
    "            predicted = torch.argmax(outputs, axis = 1)\n",
    "\n",
    "            correct_predictions += (predicted == labels_as_indices).sum().item()\n",
    "            total_predictions += labels_as_indices.size(0)\n",
    "\n",
    "            y_true.extend(labels_as_indices.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "            all_confidences.extend(confidences.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_confidences = np.array(all_confidences)\n",
    "    all_true_labels = np.array(all_true_labels)\n",
    "\n",
    "    stats = calculate_metrics_per_class(all_confidences, all_true_labels, threshold=0.5)\n",
    "\n",
    "    for class_index, class_stats in stats.items():\n",
    "        print(f\"Class {class_index}:\")\n",
    "        print(f\" Accuracy: {class_stats['accuracy']}\")\n",
    "        print(f\" Precision: {class_stats['precision']}\")\n",
    "        print(f\" Recall: {class_stats['recall']}\")\n",
    "        print(f\" F1 Score: {class_stats['f1_score']}\")\n",
    "\n",
    "    return np.mean(running_val_loss), correct_predictions, total_predictions, y_true, y_pred, stats"
   ],
   "metadata": {
    "id": "sxOpZq8vX9Sn",
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.461492Z",
     "start_time": "2024-05-18T16:59:39.458701Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pagrindnis loop'as, išsaugom istoriją kad ateityje būtų galima nubraižyt fancy grafikelius**"
   ],
   "metadata": {
    "id": "x4UCtGgqIt_n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_and_eval(model, loader_train, loader_valid, filename, epoch_count, lr):\n",
    "  loss_func = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "  early_stopping = EarlyStopping(patience=PATIENCE, min_delta=0.01)\n",
    "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=PATIENCE, verbose=True)\n",
    "  start_time = datetime.now()\n",
    "\n",
    "  training_history = None\n",
    "\n",
    "  if LOAD_CHECKPOINT:\n",
    "      training_history = load_checkpoint(filename, model, optimizer)\n",
    "      start_epoch = training_history['epoch'] + 1\n",
    "      train_loss_history = training_history['train_loss_history']\n",
    "      accuracy_history = training_history['accuracy_history']\n",
    "      train_accuracy_history = training_history['train_accuracy_history']\n",
    "      val_loss_history = training_history['val_loss_history']\n",
    "      precision_history = training_history['precision_history']\n",
    "      recall_history = training_history['recall_history']\n",
    "      f1_score_history = training_history['f1_score_history']\n",
    "      stats_history = training_history['stats_history']\n",
    "      conf_matrix = training_history['conf_matrix']\n",
    "  else:\n",
    "      start_epoch = 0\n",
    "      train_loss_history = []\n",
    "      val_loss_history = []\n",
    "      accuracy_history = []\n",
    "      train_accuracy_history = []\n",
    "      precision_history = []\n",
    "      recall_history = []\n",
    "      f1_score_history = []\n",
    "      stats_history = []\n",
    "      conf_matrix = []\n",
    "\n",
    "  for epoch in range(start_epoch, epoch_count):\n",
    "      print('Starting training epoch... ', epoch)\n",
    "      start_time = datetime.now()\n",
    "\n",
    "      train_loss, train_accuracy = train_epoch(model, loader_train, loss_func, optimizer)\n",
    "\n",
    "      current_time = datetime.now()\n",
    "      elapsed = seconds_to_time((current_time - start_time).total_seconds())\n",
    "      print(f'Epoch: {epoch}, Time: {elapsed}, Loss: {train_loss}')\n",
    "\n",
    "      print('Starting evaluation... ', start_time)\n",
    "      start_time = datetime.now()\n",
    "\n",
    "      avg_val_loss, correct_predictions, total_predictions, y_true, y_pred, stats = evaluate_epoch(model, test_loader, loss_func, optimizer)\n",
    "\n",
    "      accuracy, precision, recall, f1, conf_matrix = calculate_metrics(correct_predictions, total_predictions, y_true, y_pred)\n",
    "\n",
    "      current_time = datetime.now()\n",
    "      per_image = (current_time - start_time).total_seconds() / total_predictions\n",
    "      print(f'Time: {per_image * 1000}ms, Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 score: {f1:.4f}')\n",
    "      print(f'Confusion matrix: {conf_matrix}')\n",
    "\n",
    "      accuracy_history.append(accuracy)\n",
    "      train_accuracy_history.append(train_accuracy)\n",
    "      train_loss_history.append(train_loss)\n",
    "      val_loss_history.append(avg_val_loss)\n",
    "      precision_history.append(precision)\n",
    "      recall_history.append(recall)\n",
    "      f1_score_history.append(f1)\n",
    "      stats_history.append(stats)\n",
    "\n",
    "      print(\"Saving checkpoint...\")\n",
    "      checkpoint = {\n",
    "          \"epoch\": epoch,\n",
    "          \"state_dict\": model.state_dict(),\n",
    "          \"optimizer\": optimizer.state_dict(),\n",
    "          \"train_loss_history\": train_loss_history,\n",
    "          \"accuracy_history\": accuracy_history,\n",
    "          \"train_accuracy_history\": train_accuracy_history,\n",
    "          \"val_loss_history\": val_loss_history,\n",
    "          \"precision_history\": precision_history,\n",
    "          \"recall_history\": recall_history,\n",
    "          \"f1_score_history\": f1_score_history,\n",
    "          \"stats_history\": stats_history,\n",
    "          \"conf_matrix\": conf_matrix\n",
    "      }\n",
    "      torch.save(checkpoint, filename)\n",
    "\n",
    "      scheduler.step(avg_val_loss)\n",
    "\n",
    "      early_stopping(avg_val_loss)\n",
    "      if early_stopping.early_stop:\n",
    "          print(\"Early stopping triggered. Reducing learning rate and resetting early stopping.\")\n",
    "          early_stopping.counter = 0\n",
    "\n",
    "  return training_history\n"
   ],
   "metadata": {
    "id": "92y_R0mPQd7L",
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.466064Z",
     "start_time": "2024-05-18T16:59:39.462120Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset, test_dataset = create_datasets(ROOT_DIR, CLASS_LIST)\n",
    "\n",
    "if USE_SUBSET:\n",
    "  train_dataset = Subset(train_dataset, range(TRAIN_SUBSET_SIZE))\n",
    "  test_dataset = Subset(test_dataset, range(VALID_SUBSET_SIZE))\n",
    "\n",
    "print(f'Train: {len(train_dataset)}, Test: {len(test_dataset)}')\n",
    "\n",
    "# num_workers = 2 # Google colab\n",
    "num_workers = 6 # M3 Pro\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN81usApf92Z",
    "outputId": "c192fde3-8103-45b9-e989-931a5728f34d",
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.503398Z",
     "start_time": "2024-05-18T16:59:39.466577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/healthy\n",
      "../data/salmo\n",
      "../data/cocci\n",
      "../data/ncd\n",
      "Train: 5449, Test: 1363\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": "**VGG19_BN model modification**",
   "metadata": {
    "id": "2jDVGXtXKS4X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Užfreezinti visi parametrai išskyrus paskutinį sluoksnį**"
   ],
   "metadata": {
    "id": "G78c7hWEKo5n"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T16:59:39.505579Z",
     "start_time": "2024-05-18T16:59:39.504079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_model():\n",
    "    model = vgg.vgg19_bn(weights = vgg.VGG19_BN_Weights.DEFAULT)\n",
    "    \n",
    "    # Modify the classifier to match the number of classes\n",
    "    num_ftrs = model.classifier[6].in_features\n",
    "    model.classifier[6] = nn.Linear(num_ftrs, len(CLASS_LIST))\n",
    "    \n",
    "    return model\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "import torchvision.models.vgg as vgg\n",
    "\n",
    "model = get_model().to(device)\n",
    "\n",
    "print(f'Parameter count: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')\n",
    "training_history = train_and_eval(model, train_loader, test_loader, CHECKPOINT_FILENAME, epoch_count = EPOCH_COUNT, lr = LEARNING_RATE)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQUf3uL_QtqJ",
    "outputId": "3fe8746a-c7c5-45e9-caef-07903fcceced",
    "ExecuteTime": {
     "end_time": "2024-05-18T17:04:54.547162Z",
     "start_time": "2024-05-18T16:59:39.507478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count: 139,597,636\n",
      "Starting training epoch...  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/domantas.keturakis/Projects/PoultryDiseaseDetection/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 3m49s, Loss: 0.621783435344696\n",
      "Starting evaluation...  2024-05-18 19:59:40.097704\n",
      "Class 0:\n",
      " Accuracy: 0.6852531181217901\n",
      " Precision: 0.4897959183673469\n",
      " Recall: 0.9902912621359223\n",
      " F1 Score: 0.6554216867469879\n",
      "Class 1:\n",
      " Accuracy: 0.9112252384446075\n",
      " Precision: 0.8408163265306122\n",
      " Recall: 0.9054945054945055\n",
      " F1 Score: 0.8719576719576719\n",
      "Class 2:\n",
      " Accuracy: 0.983125458547322\n",
      " Precision: 0.9671361502347418\n",
      " Recall: 0.9786223277909739\n",
      " F1 Score: 0.9728453364817002\n",
      "Class 3:\n",
      " Accuracy: 0.9449743213499633\n",
      " Precision: 0\n",
      " Recall: 0.0\n",
      " F1 Score: 0\n",
      "Total predictions:  1363\n",
      "Correct predictions:  1180\n",
      "Time: 33.18010491562729ms, Epoch 0, Train Loss: 0.6218, Val Loss: 0.4164, Accuracy: 0.8657, Precision: 0.8248, Recall: 0.8657, F1 score: 0.8430\n",
      "Confusion matrix: [[378  34   0   0]\n",
      " [ 60 392   3   0]\n",
      " [  7   4 410   0]\n",
      " [ 53  11  11   0]]\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/domantas.keturakis/Projects/PoultryDiseaseDetection/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch...  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1206fa840>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/domantas.keturakis/Projects/PoultryDiseaseDetection/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/domantas.keturakis/Projects/PoultryDiseaseDetection/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m get_model()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mParameter count: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28msum\u001B[39m(p\u001B[38;5;241m.\u001B[39mnumel()\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mp\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mmodel\u001B[38;5;241m.\u001B[39mparameters()\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39mp\u001B[38;5;241m.\u001B[39mrequires_grad)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m training_history \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCHECKPOINT_FILENAME\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch_count\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mEPOCH_COUNT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mLEARNING_RATE\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[11], line 38\u001B[0m, in \u001B[0;36mtrain_and_eval\u001B[0;34m(model, loader_train, loader_valid, filename, epoch_count, lr)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStarting training epoch... \u001B[39m\u001B[38;5;124m'\u001B[39m, epoch)\n\u001B[1;32m     36\u001B[0m start_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[0;32m---> 38\u001B[0m train_loss, train_accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloader_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m current_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[1;32m     41\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m seconds_to_time((current_time \u001B[38;5;241m-\u001B[39m start_time)\u001B[38;5;241m.\u001B[39mtotal_seconds())\n",
      "Cell \u001B[0;32mIn[7], line 22\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, train_loader, criterion, optimizer)\u001B[0m\n\u001B[1;32m     19\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     21\u001B[0m     predicted \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(outputs, axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 22\u001B[0m     correct_predictions \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mpredicted\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlabels_as_indices\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m     total_predictions \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m labels_as_indices\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     25\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(running_train_loss)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Statistiku atvaizdavimas**"
   ],
   "metadata": {
    "id": "89VFfJK2Pdgo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = get_model().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "training_history = load_checkpoint(CHECKPOINT_FILENAME, model, optimizer)\n",
    "\n",
    "train_accuracy_history = training_history['train_accuracy_history']\n",
    "accuracy_history = training_history['accuracy_history']\n",
    "train_loss_history = training_history['train_loss_history']\n",
    "val_loss_history = training_history['val_loss_history']\n",
    "conf_matrix = training_history['conf_matrix']\n",
    "\n",
    "plt.plot(train_accuracy_history, label='Train Accuracy')\n",
    "plt.plot(accuracy_history, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "if isinstance(conf_matrix, list):\n",
    "    last_conf_matrix = np.array(conf_matrix[-1])\n",
    "else:\n",
    "    last_conf_matrix = np.array(conf_matrix)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=last_conf_matrix, display_labels=CLASS_LIST)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mFW0StkhPhb2",
    "outputId": "13b583aa-900d-4f3f-d412-09c6a4b4894c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Transformaciju atvaizdavimas**"
   ],
   "metadata": {
    "id": "ErbgYuZzge2u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def denormalize(img, mean, std):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    img = img * std + mean\n",
    "    return img\n",
    "\n",
    "def imshow(img, title=None):\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.clip(npimg, 0, 1)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "INDEX_TO_CLASS = {v: k for k, v in CLASS_INDICES.items()}\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "num_images_to_show = 5\n",
    "fig, axes = plt.subplots(1, num_images_to_show, figsize=(15, 3))\n",
    "for i in range(num_images_to_show):\n",
    "    ax = axes[i]\n",
    "    img = denormalize(images[i], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.clip(npimg, 0, 1)\n",
    "    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    class_index = labels[i].argmax().item()\n",
    "    class_name = INDEX_TO_CLASS[class_index]\n",
    "    ax.set_title(f\"Label: {class_name}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "x_qA5Cyhgimm",
    "outputId": "4dfae1d8-b5de-4bfd-8a6f-fd0ff054ce76"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
